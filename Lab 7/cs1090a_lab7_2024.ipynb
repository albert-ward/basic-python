{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS1090A Introduction to Data Science "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 7: Missingness \u0026 Classification\n",
                "### (kNN Classification, Logistic Regression, Multiple Logistic Regression)\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2024**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas \u0026 Nateshi Pillai\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sections:\n",
                "1. Data Loading \u0026 Missing Data\n",
                "2. Binary Classification: Penguin Sex\n",
                "3. Multiclass Classification: Penguin Species"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score,\\\n",
                "                                    cross_val_predict, KFold, GridSearchCV\n",
                "from sklearn.impute import KNNImputer\n",
                "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures,\\\n",
                "                                    OneHotEncoder, LabelBinarizer, label_binarize\n",
                "from sklearn.pipeline import Pipeline, make_pipeline\n",
                "from sklearn.decomposition import PCA\n",
                "# import warnings\n",
                "# warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the same random state throughout\n",
                "random_state = 209"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Binary Classification\n",
                "\n",
                "In this section we will try to predict the sex of the penguins in the now familiar peguins dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = sns.load_dataset('penguins')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "df.isna().sum(axis=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Do some rows have more than a single missing value?\n",
                "df[df.isna().sum(axis=1) \u003e 1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Yuck! These are useless. Drop these rows\n",
                "df = df[df.isna().sum(axis=1) \u003c= 1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Confirm that now sex is the only feature with missingness\n",
                "df.isna().sum(axis=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create stratification variable (NaN values will be treated as their own group)\n",
                "stratify_var = df['sex'].astype(str) + '_' + df['species']\n",
                "\n",
                "print(\"Distribution of sex-species combinations (including NaN):\")\n",
                "print(stratify_var.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Single train-test split stratifying on both sex and species\n",
                "df_train, df_test = train_test_split(\n",
                "    df,\n",
                "    test_size=0.2,\n",
                "    random_state=random_state,\n",
                "    stratify=stratify_var\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick look at class balance for sex across split\n",
                "print(\"TRAIN\")\n",
                "print(df_train['sex'].value_counts(normalize=True))\n",
                "print(\"TEST\")\n",
                "print(df_test['sex'].value_counts(normalize=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create alternate train \u0026 test dfs without missing sex\n",
                "df_train_sex = df_train.dropna(subset='sex')\n",
                "df_test_sex = df_test.dropna(subset='sex')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confirm there is no more missingness\n",
                "assert df_train_sex.isna().sum().sum() == 0\n",
                "assert df_test_sex.isna().sum().sum() == 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# X\n",
                "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
                "response = 'sex'\n",
                "X_train = df_train_sex[features]\n",
                "X_test = df_test_sex[features]\n",
                "\n",
                "# y\n",
                "# Encode sex as 0/1\n",
                "le = LabelEncoder()\n",
                "y_train = le.fit_transform(df_train_sex[response])\n",
                "y_test = le.transform(df_test_sex['sex'])\n",
                "\n",
                "# Show encoding mapping\n",
                "print(\"\\nEncoding mapping:\")\n",
                "for i, label in enumerate(le.classes_):\n",
                "    print(f\"{label} -\u003e {i}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## kNN Classifier\n",
                "\n",
                "The works just like kNN Regression except instead of predicting the average of the neighbors' $y$ value, we predict the *mode*. That is, the majority class label seen in the neighbors.\n",
                "\n",
                "**Q:** Why do we want to use scaling here?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "cv_folds = 5\n",
                "\n",
                "# Create the pipeline\n",
                "knn_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier())\n",
                "])\n",
                "    \n",
                "# Define parameter grid\n",
                "param_grid = {'knn__n_neighbors': np.arange(1, 31)}\n",
                "\n",
                "# Create KFold object\n",
                "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
                "\n",
                "# Create and fit GridSearchCV\n",
                "grid_search = GridSearchCV(\n",
                "    estimator=knn_pipeline,\n",
                "    param_grid=param_grid,\n",
                "    cv=kf,\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1,  # Use all available cores\n",
                "    return_train_score=True\n",
                ")\n",
                "\n",
                "# Fit the grid search\n",
                "print(\"Starting grid search...\")\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "# Create DataFrame with CV results\n",
                "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
                "knn_cv_acc = grid_search.best_score_\n",
                "\n",
                "# Print best parameters and score\n",
                "print(\"\\nBest parameters:\")\n",
                "for param, value in grid_search.best_params_.items():\n",
                "    print(f\"{param}: {value}\")\n",
                "print(f\"\\nBest cross-validation accuracy: {knn_cv_acc:.4f}\")\n",
                "    \n",
                "best_knn = grid_search.best_estimator_\n",
                "best_knn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Decision Boundary for kNN Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_decision_boundary(model, X, y, use_pca=False, feature1='bill_depth_mm', feature2='body_mass_g'):\n",
                "    \"\"\"\n",
                "    Plot binary decision boundary for classifier pipelines with orange and teal colors.\n",
                "    Can plot either in original feature space or PCA space.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    model : sklearn Pipeline or classifier\n",
                "        Trained pipeline (with preprocessing steps) or standalone classifier\n",
                "    X : array-like or DataFrame\n",
                "        Features used for training\n",
                "    y : array-like\n",
                "        Target values (binary)\n",
                "    use_pca : bool\n",
                "        If True, plot in PCA space using first two components\n",
                "    feature1 : str\n",
                "        Name of first feature to plot (ignored if use_pca=True)\n",
                "    feature2 : str\n",
                "        Name of second feature to plot (ignored if use_pca=True)\n",
                "    \"\"\"\n",
                "    # Ensure X is a DataFrame\n",
                "    if not isinstance(X, pd.DataFrame):\n",
                "        raise ValueError(\"X must be a pandas DataFrame\")\n",
                "    \n",
                "    if use_pca:\n",
                "        # Check if model is a pipeline with PCA\n",
                "        if isinstance(model, Pipeline) and any(isinstance(step[1], PCA) for step in model.steps):\n",
                "            # Find PCA and previous steps in the pipeline\n",
                "            pca_idx = next(i for i, (name, step) in enumerate(model.steps) \n",
                "                          if isinstance(step, PCA))\n",
                "            preprocessing_steps = Pipeline(model.steps[:pca_idx + 1])\n",
                "            classifier_steps = Pipeline(model.steps[pca_idx + 1:])\n",
                "            \n",
                "            # Transform data using preprocessing steps including PCA\n",
                "            X_pca = preprocessing_steps.transform(X)\n",
                "        else:\n",
                "            # Create and fit a new pipeline with StandardScaler and PCA just for visualization\n",
                "            viz_pipeline = Pipeline([\n",
                "                ('scaler', StandardScaler()),\n",
                "                ('pca', PCA(n_components=2))\n",
                "            ])\n",
                "            X_pca = viz_pipeline.fit_transform(X)\n",
                "            \n",
                "        # Use first two PCA components\n",
                "        x1_min, x1_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5\n",
                "        x2_min, x2_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5\n",
                "        \n",
                "        xx1, xx2 = np.meshgrid(\n",
                "            np.linspace(x1_min, x1_max, 100),\n",
                "            np.linspace(x2_min, x2_max, 100)\n",
                "        )\n",
                "        \n",
                "        # Create mesh points in PCA space\n",
                "        X_mesh = np.column_stack([xx1.ravel(), xx2.ravel()])\n",
                "        \n",
                "        if isinstance(model, Pipeline) and any(isinstance(step[1], PCA) for step in model.steps):\n",
                "            # If model has PCA, use classifier steps for prediction\n",
                "            Z = classifier_steps.predict(X_mesh)\n",
                "            if hasattr(classifier_steps, 'predict_proba'):\n",
                "                Z_proba = classifier_steps.predict_proba(X_mesh)[:, 1]\n",
                "        else:\n",
                "            # If using visualization PCA, transform mesh back to original space\n",
                "            X_mesh_original = viz_pipeline.inverse_transform(X_mesh)\n",
                "            X_mesh_df = pd.DataFrame(X_mesh_original, columns=X.columns)\n",
                "            Z = model.predict(X_mesh_df)\n",
                "            if hasattr(model, 'predict_proba'):\n",
                "                Z_proba = model.predict_proba(X_mesh_df)[:, 1]\n",
                "        \n",
                "        plot_x = X_pca[:, 0]\n",
                "        plot_y = X_pca[:, 1]\n",
                "        xlabel = 'First Principal Component'\n",
                "        ylabel = 'Second Principal Component'\n",
                "        \n",
                "    else:\n",
                "        # Original feature space plotting code\n",
                "        feature1_idx = X.columns.get_loc(feature1)\n",
                "        feature2_idx = X.columns.get_loc(feature2)\n",
                "        \n",
                "        x1_min, x1_max = X[feature1].min() - 0.5, X[feature1].max() + 0.5\n",
                "        x2_min, x2_max = X[feature2].min() - 0.5, X[feature2].max() + 0.5\n",
                "        \n",
                "        xx1, xx2 = np.meshgrid(\n",
                "            np.linspace(x1_min, x1_max, 100),\n",
                "            np.linspace(x2_min, x2_max, 100)\n",
                "        )\n",
                "        \n",
                "        feature_means = X.mean()\n",
                "        mesh_shape = xx1.ravel().shape[0]\n",
                "        mesh_dict = {col: np.repeat(feature_means[col], mesh_shape) for col in X.columns}\n",
                "        mesh_dict[feature1] = xx1.ravel()\n",
                "        mesh_dict[feature2] = xx2.ravel()\n",
                "        \n",
                "        X_mesh = pd.DataFrame(mesh_dict, columns=X.columns)\n",
                "        Z = model.predict(X_mesh)\n",
                "        if hasattr(model, 'predict_proba'):\n",
                "            Z_proba = model.predict_proba(X_mesh)[:, 1]\n",
                "            \n",
                "        plot_x = X[feature1]\n",
                "        plot_y = X[feature2]\n",
                "        xlabel = feature1\n",
                "        ylabel = feature2\n",
                "    \n",
                "    Z_binary = Z.reshape(xx1.shape)\n",
                "    \n",
                "    # Create the plot\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    \n",
                "    # Define colors\n",
                "    region_colors = ['#FFE5CC', '#CCE5E5']  # Light orange, Light teal\n",
                "    scatter_colors = ['#FF8C1A', '#008080']  # Darker orange, Darker teal\n",
                "    \n",
                "    # Plot decision boundary\n",
                "    plt.contourf(xx1, xx2, Z_binary, alpha=0.3, levels=[-1, 0.5, 1], \n",
                "                colors=region_colors)\n",
                "    \n",
                "    # If model has predict_proba, plot probability contours\n",
                "    if hasattr(model, 'predict_proba') or (isinstance(model, Pipeline) and \n",
                "            hasattr(model.steps[-1][1], 'predict_proba')):\n",
                "        Z_proba = Z_proba.reshape(xx1.shape)\n",
                "        plt.contour(xx1, xx2, Z_proba, levels=[0.5], colors='black', linewidths=2)\n",
                "    \n",
                "    # Plot the points\n",
                "    plt.scatter(\n",
                "        plot_x,\n",
                "        plot_y,\n",
                "        c=[scatter_colors[int(val)] for val in y],\n",
                "        alpha=0.8\n",
                "    )\n",
                "    \n",
                "    # Create legend\n",
                "    legend_elements = [\n",
                "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=scatter_colors[0], \n",
                "                  label='Female', markersize=10),\n",
                "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=scatter_colors[1], \n",
                "                  label='Male', markersize=10)\n",
                "    ]\n",
                "    plt.legend(handles=legend_elements, title='Sex')\n",
                "    \n",
                "    # Get model name\n",
                "    if isinstance(model, Pipeline):\n",
                "        classifier = model.steps[-1][1]\n",
                "        model_name = (\n",
                "            'Logistic Regression' if isinstance(classifier, LogisticRegression)\n",
                "            else 'K-Nearest Neighbors' if isinstance(classifier, KNeighborsClassifier)\n",
                "            else 'Classifier'\n",
                "        )\n",
                "    else:\n",
                "        model_name = (\n",
                "            'Logistic Regression' if isinstance(model, LogisticRegression)\n",
                "            else 'K-Nearest Neighbors' if isinstance(model, KNeighborsClassifier)\n",
                "            else 'Classifier'\n",
                "        )\n",
                "    \n",
                "    plt.xlabel(xlabel)\n",
                "    plt.ylabel(ylabel)\n",
                "    title = f'{model_name} Decision Boundary'\n",
                "    if use_pca:\n",
                "        title += ' (PCA Space)'\n",
                "    plt.title(title)\n",
                "    \n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_decision_boundary(best_knn, X_train, y_train, use_pca=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Why not Linear Regression for Classification?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_linear_to_probability():\n",
                "    x = np.linspace(-3, 3, 100)\n",
                "    betas = [0.5, 1.0, 2.0]\n",
                "    \n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
                "    \n",
                "    for beta in betas:\n",
                "        z = beta * x\n",
                "        p = 1/(1 + np.exp(-z))\n",
                "        \n",
                "        ax1.plot(x, z, label=f'β = {beta}')\n",
                "        ax2.plot(x, p, label=f'β = {beta}')\n",
                "    \n",
                "    ax1.set_title('Linear Combination (z)')\n",
                "    ax1.set_xlabel('x')\n",
                "    ax1.set_ylabel('z = βx')\n",
                "    ax1.legend()\n",
                "    ax1.grid(True)\n",
                "    \n",
                "    ax2.set_title('Probability P(Y=1|X)')\n",
                "    ax2.set_xlabel('x')\n",
                "    ax2.set_ylabel('p(x)')\n",
                "    ax2.legend()\n",
                "    ax2.grid(True)\n",
                "\n",
                "    plt.tight_layout()\n",
                "\n",
                "plot_linear_to_probability()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_logistic_visualization():\n",
                "    x = np.linspace(-6, 6, 100)\n",
                "    p = 1/(1 + np.exp(-x))\n",
                "    \n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
                "    \n",
                "    # Plot logistic function\n",
                "    ax1.plot(x, p)\n",
                "    ax1.set_title('Logistic Function')\n",
                "    ax1.set_xlabel('Log Odds')\n",
                "    ax1.set_ylabel('Probability')\n",
                "    ax1.grid(True)\n",
                "    \n",
                "    # Plot log odds vs probability\n",
                "    p_range = np.linspace(0.01, 0.99, 100)\n",
                "    log_odds = np.log(p_range/(1-p_range))\n",
                "    \n",
                "    ax2.plot(p_range, log_odds)\n",
                "    ax2.set_title('Log Odds vs Probability')\n",
                "    ax2.set_xlabel('Probability')\n",
                "    ax2.set_ylabel('Log Odds')\n",
                "    ax2.grid(True)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    return fig\n",
                "\n",
                "fig = create_logistic_visualization()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Maximum Likelihood Estimation → Loss Functions\n",
                "\n",
                "### Binary Case: Y ~ Bernoulli\n",
                "\n",
                "#### 1. Model Assumption\n",
                "For each observation i, assume Yi follows a Bernoulli distribution:\n",
                "$$ Y_i \\sim \\text{Bernoulli}(p_i) $$\n",
                "\n",
                "where pi is a function of our predictors:\n",
                "$$ p_i = P(Y_i=1|X_i) = \\frac{1}{1 + e^{-\\beta^T x_i}} $$\n",
                "\n",
                "#### 2. Likelihood Function\n",
                "The probability mass function for a single observation is:\n",
                "$$ P(Y_i = y_i) = p_i^{y_i}(1-p_i)^{1-y_i} $$\n",
                "\n",
                "Assuming independent observations, the likelihood for all observations is:\n",
                "$$ L(\\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i} $$\n",
                "\n",
                "#### 3. Log-Likelihood\n",
                "Taking the log (to convert product to sum):\n",
                "$$ \\ell(\\beta) = \\sum_{i=1}^n [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)] $$\n",
                "\n",
                "#### 4. Substitute Model for pi\n",
                "$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[y_i\\log\\left(\\frac{1}{1 + e^{-\\beta^T x_i}}\\right) + (1-y_i)\\log\\left(1 - \\frac{1}{1 + e^{-\\beta^T x_i}}\\right)\\right] $$\n",
                "\n",
                "#### 5. Loss Function\n",
                "The negative log-likelihood (our loss function) is:\n",
                "$$ L(\\beta) = -\\ell(\\beta) = -\\sum_{i=1}^n [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)] $$\n",
                "\n",
                "This is the binary cross-entropy loss!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Logistic Regression with SKLearn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create basic logistic regression without scaling\n",
                "lr = LogisticRegression(random_state=random_state,\n",
                "                        penalty=None,\n",
                "                        max_iter=1000)\n",
                "\n",
                "# Fit the model on all training data\n",
                "lr.fit(X_train, y_train)\n",
                "\n",
                "# Get cross validation scores using the same KFold object\n",
                "lr_cv_scores = cross_val_score(\n",
                "    lr, \n",
                "    X_train, \n",
                "    y_train, \n",
                "    cv=kf,\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "# Calculate mean CV accuracy\n",
                "lr_cv_acc = lr_cv_scores.mean()\n",
                "\n",
                "print(f\"Cross-validation accuracy: {lr_cv_acc:.4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Interpreting the Coefficients\n",
                "\n",
                "Let's inspect the coefficients of our first logistic regression model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's interpret the unregularized model's coefficients\n",
                "unreg_coef_df = pd.DataFrame({\n",
                "    'Feature': features,\n",
                "    'Coefficient': lr.coef_[0],  # [0] because binary classification returns 1 row\n",
                "    'Odds_Ratio': np.exp(lr.coef_[0])\n",
                "})\n",
                "unreg_coef_df = unreg_coef_df.sort_values('Coefficient', ascending=False)\n",
                "\n",
                "print(\"Unregularized Model Coefficients:\")\n",
                "print(unreg_coef_df)\n",
                "print(\"\\nIntercept:\", lr.intercept_[0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remember, the logistic regression coefficients describe a linear relationship between the features and the *log-odds*, not the probability!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LASSO Regularized Logistic Regression\n",
                "\n",
                "For logistic regression models, SKLearn calls the regularization `C`. **Be careful! This is $\\frac{1}{\\lambda}$.** So a larger `C` means a smaller penalty and thus *less* regularization.\n",
                "\n",
                "If we want to use LASSO we need to set `pentaly='l1'`. The default solver not work with LASSO so we need to set `solver='liblinear'`.\n",
                "\n",
                "**Q:** Why are we scaling here when we didn't in the previous model?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pipeline with scaling and LogisticRegressionCV\n",
                "Cs = np.logspace(-3, 3, 100)\n",
                "reg_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('lr_cv', LogisticRegressionCV(\n",
                "        Cs=Cs,\n",
                "        penalty='l1',\n",
                "        cv=kf,  # Using same KFold object for fair comparison\n",
                "        scoring='accuracy',\n",
                "        random_state=random_state,\n",
                "        solver='liblinear',\n",
                "        n_jobs=-1\n",
                "    ))\n",
                "])\n",
                "\n",
                "# Fit the model\n",
                "print(\"Fitting regularized model...\")\n",
                "reg_pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Get the best C value and CV accuracy\n",
                "best_C = reg_pipeline.named_steps['lr_cv'].C_[0]  # [0] for binary classification\n",
                "reg_cv_acc = reg_pipeline.named_steps['lr_cv'].scores_[1].mean(axis=0).max()\n",
                "\n",
                "print(f\"\\nBest C: {best_C:.4f}\")\n",
                "print(f\"CV accuracy: {reg_cv_acc:.4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Interpreting the Regularized Coefficients\n",
                "\n",
                "The feature scaling can confuse interpetation if you aren't careful!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For regularized model\n",
                "scaler = reg_pipeline.named_steps['scaler']\n",
                "reg_lr = reg_pipeline.named_steps['lr_cv']\n",
                "\n",
                "# Get standardized coefficients for feature importance\n",
                "standardized_coef = reg_lr.coef_[0]  # These are already in standardized scale\n",
                "\n",
                "# Get original scale coefficients for interpretation\n",
                "original_scale_coef = standardized_coef / scaler.scale_\n",
                "\n",
                "# Create comparison DataFrame with both scales\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Feature': features,\n",
                "    'Standardized_Coef': standardized_coef,\n",
                "    'Original_Scale_Coef': original_scale_coef,\n",
                "    'Original_Scale_Odds': np.exp(original_scale_coef)\n",
                "})\n",
                "\n",
                "# Sort by absolute standardized coefficient for feature importance\n",
                "comparison_df['Abs_Standardized_Coef'] = abs(comparison_df['Standardized_Coef'])\n",
                "comparison_df = comparison_df.sort_values('Abs_Standardized_Coef', ascending=False).reset_index(drop=True)\n",
                "\n",
                "print(\"\\nFeature Importance and Interpretation:\")\n",
                "comparison_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Comparison to the unregularized coefficients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize coefficient comparison\n",
                "plt.figure(figsize=(12, 6))\n",
                "x = np.arange(len(features))\n",
                "width = 0.35\n",
                "\n",
                "plt.bar(x - width/2, lr.coef_[0], width, label='Unregularized')\n",
                "plt.bar(x + width/2, standardized_coef, width, label='Regularized (scaled)')\n",
                "plt.xticks(x, features, rotation=45, ha='right')\n",
                "plt.xlabel('Features')\n",
                "plt.ylabel('Coefficient Value')\n",
                "plt.title('Comparison of Regularized vs Unregularized Scaled Coefficients')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_2_features = comparison_df.loc[:1, \"Feature\"].values\n",
                "best_2_features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "USE_BEST_2_FEATURES = False\n",
                "features = best_2_features if USE_BEST_2_FEATURES else ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_2features = LogisticRegression(penalty=None, random_state=random_state).fit(X_train[features], y_train)\n",
                "random_state = 209\n",
                "lr_2features.intercept_, lr_2features.coef_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_decision_boundary(lr_2features, X_train[features], y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regularized Polynomial Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pipeline with polynomial features, scaling, and Lasso logistic regression\n",
                "max_iter = 1000\n",
                "poly_lasso_pipeline = Pipeline([\n",
                "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('lr_cv', LogisticRegressionCV(\n",
                "        Cs=np.logspace(-3, 3, 20),\n",
                "        cv=kf,\n",
                "        scoring='accuracy',\n",
                "        penalty='l1',\n",
                "        solver='liblinear',  # Required for L1 penalty\n",
                "        random_state=random_state,\n",
                "        max_iter=max_iter,\n",
                "        n_jobs=-1\n",
                "    ))\n",
                "])\n",
                "\n",
                "# Fit the model\n",
                "print(\"Fitting Lasso model with polynomial features...\")\n",
                "poly_lasso_pipeline.fit(X_train[features], y_train)\n",
                "\n",
                "# Get CV accuracy\n",
                "lasso_cv_acc = poly_lasso_pipeline.named_steps['lr_cv'].scores_[1].mean(axis=0).max()\n",
                "print(f\"\\nBest CV accuracy: {lasso_cv_acc:.4f}\")\n",
                "print(f\"Best C: {poly_lasso_pipeline.named_steps['lr_cv'].C_[0]:.4f}\")\n",
                "\n",
                "# Get feature names from polynomial transformation\n",
                "feature_names_poly = poly_lasso_pipeline.named_steps['poly'].get_feature_names_out(X_train[features].columns)\n",
                "\n",
                "# Get coefficients\n",
                "lasso_coef = poly_lasso_pipeline.named_steps['lr_cv'].coef_[0]\n",
                "\n",
                "# Create DataFrame of features and their coefficients\n",
                "coef_df = pd.DataFrame({\n",
                "    'Feature': feature_names_poly,\n",
                "    'Coefficient': lasso_coef\n",
                "})\n",
                "\n",
                "# Sort by absolute coefficient value and show non-zero coefficients\n",
                "coef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])\n",
                "coef_df = coef_df[coef_df['Coefficient'] != 0].sort_values('Abs_Coefficient', ascending=False)\n",
                "\n",
                "print(\"\\nNon-zero coefficients:\")\n",
                "print(coef_df[['Feature', 'Coefficient']].to_string())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Non-linear Decision Boundaries (in original feature space)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_decision_boundary(poly_lasso_pipeline, X_train[features], y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ROC Curves \u0026 AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_roc_curves_binary(models, X, y, cv=None):\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    \n",
                "    # Dictionary to store AUC for each model\n",
                "    aucs = {}\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        if cv is not None:\n",
                "            # Get cross-validated probabilities\n",
                "            y_proba = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n",
                "        else:\n",
                "            y_proba = model.predict_proba(X)\n",
                "        \n",
                "        # Calculate ROC curve and ROC area using probabilities of class 1\n",
                "        fpr, tpr, _ = roc_curve(y, y_proba[:, 1])\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        aucs[name] = roc_auc\n",
                "        \n",
                "        # Plot ROC curve for this model\n",
                "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
                "    \n",
                "    # Plot diagonal line\n",
                "    plt.plot([0, 1], [0, 1], 'k--')\n",
                "    plt.xlim([0.0, 1.0])\n",
                "    plt.ylim([0.0, 1.05])\n",
                "    plt.xlabel('False Positive Rate')\n",
                "    plt.ylabel('True Positive Rate')\n",
                "    if cv is not None:\n",
                "        subtitle = 'Cross-Validated'\n",
                "    # Assume we are using test data if not cross validating\n",
                "    else:\n",
                "        subtitle = 'Test'\n",
                "    plt.title(f'ROC Curves ({subtitle})')\n",
                "    plt.legend(loc=\"lower right\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "    \n",
                "    return aucs"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cross-Validated ROC curves and AUC scores\n",
                "\n",
                "ROC curves and AUC (area under the cuve) scores help us understand how well our model will perform across all possible decision thresholds, not just 0.5!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'KNN': best_knn,\n",
                "    'Ridge LogReg': reg_pipeline,\n",
                "    'Lasso Poly LogReg': poly_lasso_pipeline\n",
                "}\n",
                "\n",
                "# Plot ROC curves and get AUC scores using training data\n",
                "print(\"\\nGenerating cross-validated ROC curves...\")\n",
                "aucs = plot_roc_curves_binary(models, X_train, y_train, cv=kf)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Performance\n",
                "\n",
                "We can inspect the final model performance here, but we really should only perform model selection based on the cross-validation results!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nGenerating ROC curves...\")\n",
                "aucs = plot_roc_curves_binary(models, X_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Select a Model to Use as Imputer\n",
                "\n",
                "We can use our binary classifier to impute the missing sex values in the original DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "def impute_sex(df_to_impute, imputation_model, features_for_prediction):\n",
                "    df_imputed = df_to_impute.copy()\n",
                "    missing_sex_mask = df_imputed['sex'].isna()\n",
                "        \n",
                "    if missing_sex_mask.sum() \u003e 0:\n",
                "        # Get the features for rows with missing sex\n",
                "        X_missing = df_imputed.loc[missing_sex_mask, features_for_prediction]\n",
                "        \n",
                "        # Predict sex for missing values\n",
                "        predicted_sex = imputation_model.predict(X_missing)\n",
                "        \n",
                "        # Impute the predicted values\n",
                "        df_imputed.loc[missing_sex_mask, 'sex'] = predicted_sex\n",
                "        \n",
                "        # Create a column for missingness indicator\n",
                "        df_imputed['sex_imputed'] = missing_sex_mask\n",
                "        \n",
                "        # Print imputation summary\n",
                "        print(f\"Imputed {missing_sex_mask.sum()} missing sex values\")\n",
                "        \n",
                "        # Check for any remaining missing values\n",
                "        assert df_imputed['sex'].isna().sum() == 0, \"There are still missing values after imputation\"\n",
                "            \n",
                "    else:\n",
                "        print(\"No missing sex values found in the dataset\")\n",
                "        \n",
                "    return df_imputed\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train_imputed = impute_sex(df_train, poly_lasso_pipeline, features_for_prediction=features)\n",
                "df_test_imputed = impute_sex(df_test, poly_lasso_pipeline, features_for_prediction=features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train_imputed.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train_imputed.isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train_imputed['sex'] = df_train_imputed['sex'].replace({'Female': 0, 'Male': 1}).astype(int)\n",
                "df_test_imputed['sex'] = df_test_imputed['sex'].replace({'Female': 0, 'Male': 1}).astype(int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train_imputed"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Predicting Species"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multiclass Case: Y ~ Multinomial\n",
                "\n",
                "#### 1. Model Assumption\n",
                "For each observation i, assume Yi follows a Multinomial distribution (with n=1):\n",
                "$$ Y_i \\sim \\text{Multinomial}(1, p_{i1},...,p_{iK}) $$\n",
                "\n",
                "where for each class k:\n",
                "$$ p_{ik} = P(Y_i=k|X_i) = \\frac{e^{\\beta_k^T x_i}}{\\sum_{j=1}^K e^{\\beta_j^T x_i}} $$\n",
                "\n",
                "#### 2. Likelihood Function\n",
                "For one observation:\n",
                "$$ P(Y_i) = \\prod_{k=1}^K p_{ik}^{I(y_i=k)} $$\n",
                "\n",
                "For all observations:\n",
                "$$ L(\\beta) = \\prod_{i=1}^n \\prod_{k=1}^K p_{ik}^{I(y_i=k)} $$\n",
                "\n",
                "#### 3. Log-Likelihood\n",
                "$$ \\ell(\\beta) = \\sum_{i=1}^n \\sum_{k=1}^K I(y_i=k)\\log(p_{ik}) $$\n",
                "\n",
                "#### 4. Substitute Model for pik\n",
                "$$ \\ell(\\beta) = \\sum_{i=1}^n \\sum_{k=1}^K I(y_i=k)\\log\\left(\\frac{e^{\\beta_k^T x_i}}{\\sum_{j=1}^K e^{\\beta_j^T x_i}}\\right) $$\n",
                "\n",
                "#### 5. Loss Function\n",
                "$$ L(\\beta) = -\\ell(\\beta) = -\\sum_{i=1}^n \\sum_{k=1}^K I(y_i=k)\\log(p_{ik}) $$\n",
                "\n",
                "This is the multiclass cross-entropy loss!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set up our predictors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'sex_imputed']\n",
                "response = ['species']\n",
                "X_train = df_train_imputed[features]\n",
                "X_test = df_test_imputed[features]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And the response (which needs to be encoded)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "def encode_species_multiple_ways(df_train, df_test, response_col='species'):\n",
                "    \"\"\"\n",
                "    Demonstrate different ways to encode species for multiclass classification\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    df_train : pandas.DataFrame\n",
                "        Training data containing species column\n",
                "    df_test : pandas.DataFrame\n",
                "        Test data containing species column\n",
                "    response_col : str\n",
                "        Name of the species column\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    dict : Dictionary containing different encodings\n",
                "    \"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    # 1. Label Encoding (0, 1, 2)\n",
                "    le = LabelEncoder()\n",
                "    y_train_label = le.fit_transform(df_train[response_col])\n",
                "    y_test_label = le.transform(df_test[response_col])\n",
                "    \n",
                "    results['label_encoding'] = {\n",
                "        'train': y_train_label,\n",
                "        'test': y_test_label,\n",
                "        'classes': le.classes_,\n",
                "        'le': le,\n",
                "        'mapping': {label: idx for idx, label in enumerate(le.classes_)}\n",
                "    }\n",
                "    \n",
                "    # 2. One-Hot Encoding (sparse matrix by default)\n",
                "    ohe = OneHotEncoder(sparse_output=False)\n",
                "    y_train_onehot = ohe.fit_transform(df_train[[response_col]])\n",
                "    y_test_onehot = ohe.transform(df_test[[response_col]])\n",
                "    \n",
                "    results['onehot_encoding'] = {\n",
                "        'train': y_train_onehot,\n",
                "        'test': y_test_onehot,\n",
                "        'feature_names': ohe.get_feature_names_out([response_col]),\n",
                "        'categories': ohe.categories_[0]\n",
                "    }\n",
                "    \n",
                "    # 3. Label Binarizer (dense matrix)\n",
                "    lb = LabelBinarizer()\n",
                "    y_train_binary = lb.fit_transform(df_train[response_col])\n",
                "    y_test_binary = lb.transform(df_test[response_col])\n",
                "    \n",
                "    results['label_binarizer'] = {\n",
                "        'train': y_train_binary,\n",
                "        'test': y_test_binary,\n",
                "        'classes': lb.classes_\n",
                "    }\n",
                "    \n",
                "    return results\n",
                "\n",
                "def print_encoding_examples(encodings, sample_species):\n",
                "    \"\"\"Print examples of each encoding type\"\"\"\n",
                "    print(\"Example encodings for species:\", sample_species)\n",
                "    print(\"\\n1. Label Encoding:\")\n",
                "    print(\"Mapping:\", encodings['label_encoding']['mapping'])\n",
                "    \n",
                "    print(\"\\n2. One-Hot Encoding:\")\n",
                "    print(\"Feature names:\", encodings['onehot_encoding']['feature_names'])\n",
                "    print(\"Example array shape:\", encodings['onehot_encoding']['train'].shape)\n",
                "    print(\"Example row:\", encodings['onehot_encoding']['train'][0])\n",
                "    \n",
                "    print(\"\\n3. Label Binarizer:\")\n",
                "    print(\"Classes:\", encodings['label_binarizer']['classes'])\n",
                "    print(\"Example array shape:\", encodings['label_binarizer']['train'].shape)\n",
                "    print(\"Example row:\", encodings['label_binarizer']['train'][0])\n",
                "\n",
                "# Encode species using all methods\n",
                "encodings = encode_species_multiple_ways(df_train_imputed, df_test_imputed)\n",
                "\n",
                "# Print examples\n",
                "sample_species = df_train_imputed['species'].iloc[0]\n",
                "print_encoding_examples(encodings, sample_species)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose your preferred encoding:\n",
                "y_train = encodings['label_encoding']['train']\n",
                "y_test = encodings['label_encoding']['test']\n",
                "le = encodings['label_encoding']['le']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize empty DataFrame for scores\n",
                "scores_df = pd.DataFrame(columns=['Model', 'Train Score', 'CV Score', 'Validation AUC'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multiclass with kNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "# KNN with GridSearchCV\n",
                "print(\"Fitting KNN model...\")\n",
                "knn_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier())\n",
                "])\n",
                "\n",
                "knn_params = {\n",
                "    'knn__n_neighbors': [1, 2, 3, 4, 5, 7, 9, 11],\n",
                "}\n",
                "\n",
                "knn_grid = GridSearchCV(knn_pipeline, knn_params, cv=kf, scoring='accuracy')\n",
                "knn_grid.fit(X_train, y_train)\n",
                "\n",
                "# Get KNN scores\n",
                "knn_train_score = knn_grid.score(X_train, y_train)\n",
                "knn_cv_score = knn_grid.best_score_\n",
                "\n",
                "print(f\"\\nBest KNN parameters: {knn_grid.best_params_}\")\n",
                "print(f\"KNN Train score: {knn_train_score:.4f}\")\n",
                "print(f\"KNN CV score: {knn_cv_score:.4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multiclass Ridge Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ridge Logistic Regression (on original features)\n",
                "print(\"\\nFitting Ridge Logistic Regression model...\")\n",
                "ridge_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('logistic', LogisticRegressionCV(\n",
                "        Cs=Cs,\n",
                "        cv=kf,\n",
                "        penalty='l2',\n",
                "        solver='lbfgs',\n",
                "        max_iter=1000,\n",
                "        scoring='accuracy'\n",
                "    ))\n",
                "])\n",
                "\n",
                "ridge_pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Get Ridge scores\n",
                "ridge_train_score = ridge_pipeline.score(X_train, y_train)\n",
                "ridge_cv_score = ridge_pipeline.named_steps['logistic'].scores_[1].mean()\n",
                "\n",
                "print(f\"Ridge Logistic Regression Train score: {ridge_train_score:.4f}\")\n",
                "print(f\"Ridge Logistic Regression CV score: {ridge_cv_score:.4f}\")\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lasso Logistic Regression with Polynomial Features\n",
                "print(\"\\nFitting Lasso Polynomial Logistic Regression model...\")\n",
                "poly_lasso_pipeline = Pipeline([\n",
                "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('logistic', LogisticRegressionCV(\n",
                "        Cs=Cs,\n",
                "        cv=kf,\n",
                "        penalty='l1',\n",
                "        solver='saga', # our only solver option for l1 multiclass! See docs\n",
                "        max_iter=10000,\n",
                "        scoring='accuracy'\n",
                "    ))\n",
                "])\n",
                "\n",
                "poly_lasso_pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Get Lasso scores\n",
                "lasso_train_score = poly_lasso_pipeline.score(X_train, y_train)\n",
                "lasso_cv_score = poly_lasso_pipeline.named_steps['logistic'].scores_[1].mean()\n",
                "\n",
                "print(f\"Lasso Polynomial Logistic Regression Train score: {lasso_train_score:.4f}\")\n",
                "print(f\"Lasso Polynomial Logistic Regression CV score: {lasso_cv_score:.4f}\")\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hmm... which model to select?\n",
                "\n",
                "The Lasso model seems to be overfitting given the big difference between training and cross-validation scores.\n",
                "\n",
                "kNN had the highest CV score, but the training accuracy is 100%. Not surprising given it selected $k=1$ as the best $k$! That model seems very prone to overfitting as well, despite what the validation error may suggest.\n",
                "\n",
                "The Ridge model still had relatively high CV accuracy that is not too different from its training performance, suggesting it is not (too) over fit. Let's go with the Ridge model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate selected model on test data\n",
                "ridge_pipeline.score(X_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Wowee! 😲"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
